# informationTheoryToolkit1
 Made for calculating and analyzing various information theory metrics for discrete random variables.

 Project
 
• a function called "entropy" which computes the entropy of a discrete random variable given its probability mass function [p1; p2; :::; pN].

• a script called "test entropy2" which computes the entropy for a
generic binary random variable as a function of p0 and plots the entropy
function.

• a function called "joint entropy" which computes the joint entropy of two generic discrete random variables given their joint p.m.f.

• a function called "conditional entropy" which computes the conditional entropy of two generic discrete random variables given their
joint and marginal p.m.f.

• a function called "mutual information" which computes the mutual information of two generic discrete random variables given their
joint and marginal p.m.f.

• the functions for normalized versions of conditional entropy, joint entropy and mutual information for the discrete case.
